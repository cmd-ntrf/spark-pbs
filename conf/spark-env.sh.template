#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# This file is sourced when running various Spark programs.
# Copy it as spark-env.sh and edit that (search for <FILL_IN>) to configure Spark for your site.

# Options read by executors and drivers running inside the cluster
# - SPARK_CLASSPATH, default classpath entries to append
# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data

# Options for the daemons used in the standalone deploy mode
# - SPARK_WORKER_DIR, to set the working directory of worker processes

# Generic options for the daemons used in the standalone deploy mode
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)

export JAVA_HOME="<FILL_IN>"
export SPARK_LOCAL_DIRS="<FILL_IN>"
export SPARK_WORKER_DIR="<FILL_IN>"

# Save Spark log where the job was started
if [[ -w "${PBS_O_WORKDIR}" ]]; then
    export SPARK_LOG_DIR="${PBS_O_WORKDIR}/spark_${VERSION}_logs"
else
    export SPARK_LOG_DIR="/tmp"
fi
